{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b772115f794890",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T09:37:36.879882Z",
     "start_time": "2024-11-10T09:37:30.012449Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8d6ba10518d3df",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T09:38:09.513340Z",
     "start_time": "2024-11-10T09:38:09.507829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "index_name = \"recursive-index\"\n",
    "chunk_size = 200\n",
    "embedding_model = \"all-MiniLM-L6-v2\"\n",
    "data_path = r\"../src/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-10T09:38:11.187109Z",
     "start_time": "2024-11-10T09:38:10.298958Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/RagTermsAndServices/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load API keys\n",
    "with open(r\"../src/api_keys.json\") as f:\n",
    "    api_keys = json.load(f)\n",
    "    PINECONE_API_KEY = api_keys[\"pinecone\"]\n",
    "\n",
    "# Load the models\n",
    "sentence_transformer_model = SentenceTransformer(embedding_model)\n",
    "dimension = sentence_transformer_model.get_sentence_embedding_dimension() \n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(\n",
    "        api_key=PINECONE_API_KEY\n",
    "    )\n",
    "def create_index(pc, index_name, dimension):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c75e0f0c80c3119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T17:23:44.204303Z",
     "start_time": "2024-10-17T17:23:44.188321Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def load_data(directory: str):\n",
    "    \"\"\"Load all text files from a directory and its subdirectories.\"\"\"\n",
    "    documents = []\n",
    "    company_names = []\n",
    "    for foldername, _, filenames in os.walk(directory):\n",
    "        company_name = os.path.basename(foldername)\n",
    "        if company_name == 'dataset':\n",
    "            continue\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".md\"):\n",
    "                filepath = os.path.join(foldername, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    documents.append(f.read())\n",
    "                    company_names.append(company_name)  # Add company name for each document\n",
    "    return documents, company_names\n",
    "\n",
    "def overlapping_chunking(documents: list, chunk_size: int, overlap_size: int, company_names: list):\n",
    "    \"\"\"Create overlapping chunks based on word count and prepend company names.\"\"\"\n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Split the document into words\n",
    "        words = doc.split()\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if chunk:  # Ensure the chunk is not empty\n",
    "                chunks.append(chunk.strip())\n",
    "                chunks_company.append(company)\n",
    "                \n",
    "    return chunks, chunks_company\n",
    "\n",
    "def semantic_chunking(documents: list, company_names: list, model_embedding_name: str):\n",
    "    \"\"\"Split documents into smaller chunks using semantic chunking and prepend company names.\"\"\"\n",
    "    \n",
    "    # Create HuggingFaceEmbeddings wrapper for SentenceTransformer\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_embedding_name)\n",
    "    \n",
    "    # Create SemanticChunker\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings,\n",
    "        breakpoint_threshold_type='percentile',\n",
    "        breakpoint_threshold_amount=90\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Use SemanticChunker to split the document\n",
    "        doc_chunks = text_splitter.split_text(doc)\n",
    "        \n",
    "        # Add chunks and corresponding company names\n",
    "        chunks.extend(doc_chunks)\n",
    "        chunks_company.extend([company] * len(doc_chunks))\n",
    "    \n",
    "    return chunks, chunks_company\n",
    "\n",
    "def spacy_chunking(documents: list, company_names: list, chunk_size: int=1000, overlap_size: int=500):\n",
    "    \"\"\"Split documents into smaller chunks using SpacyTextSplitter and prepend company names.\"\"\"\n",
    "    \n",
    "    # Create SpacyTextSplitter\n",
    "    text_splitter = SpacyTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap_size\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Use SpacyTextSplitter to split the document\n",
    "        doc_chunks = text_splitter.split_text(doc)\n",
    "        \n",
    "        # Add chunks and corresponding company names\n",
    "        chunks.extend(doc_chunks)\n",
    "        chunks_company.extend([company] * len(doc_chunks))\n",
    "    \n",
    "    return chunks, chunks_company\n",
    "\n",
    "def recursive_chunking(documents: list, company_names: list, chunk_size: int=1000, overlap_size: int=500):\n",
    "    \"\"\"Split documents into smaller chunks using RecursiveCharacterTextSplitter and prepend company names.\"\"\"\n",
    "    \n",
    "    # Create RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap_size\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Use RecursiveCharacterTextSplitter to split the document\n",
    "        doc_chunks = text_splitter.split_text(doc)\n",
    "        \n",
    "        # Add chunks and corresponding company names\n",
    "        chunks.extend(doc_chunks)\n",
    "        chunks_company.extend([company] * len(doc_chunks))\n",
    "    \n",
    "    return chunks, chunks_company\n",
    "\n",
    "def embed_text(texts: list):\n",
    "    \"\"\"Embed texts using either Cohere or SentenceTransformer.\"\"\"\n",
    "    return sentence_transformer_model.encode(texts, convert_to_tensor=True).tolist()\n",
    "\n",
    "def upsert_index(index, embeddings, metadata, company_names, batch_size=100):\n",
    "    \"\"\"Insert embeddings into Pinecone in batches with metadata.\"\"\"\n",
    "    batch = []\n",
    "    \n",
    "    for idx, (emb, md, cn) in enumerate(zip(embeddings, metadata, company_names)):\n",
    "        vector = {\"id\": str(idx), \"values\": emb, \"metadata\": {\"text\": md, \"company_name\": cn}}\n",
    "        batch.append(vector)\n",
    "        \n",
    "        # When batch is full, upsert it\n",
    "        if len(batch) == batch_size:\n",
    "            index.upsert(vectors=batch)\n",
    "            batch = []  # Clear the batch\n",
    "\n",
    "    # Upsert any remaining vectors\n",
    "    if batch:\n",
    "        index.upsert(vectors=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3997f8dbda8a5bd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T17:24:01.030568Z",
     "start_time": "2024-10-17T17:24:00.160447Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and embedd Data\n",
    "documents, company_names = load_data(data_path)   \n",
    "chunks, company_names_chunks = recursive_chunking(documents, company_names)\n",
    "embeddings = embed_text(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f87929876fc166e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T11:35:25.279928Z",
     "start_time": "2024-10-12T11:34:17.209284Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# index the data\n",
    "create_index(pc, index_name, dimension)\n",
    "index = pc.Index(index_name)\n",
    "upsert_index(index, embeddings, chunks, company_names_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
