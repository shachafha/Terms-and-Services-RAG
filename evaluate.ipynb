{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"### Task Description:\n",
    "An instruction (might include an input inside it), a response to evaluate, a reference answer that receives a score of 5, and a score rubric representing multiple evaluation criteria are provided.\n",
    "\n",
    "1. Write specific and constructive feedback that assesses the responseâ€™s quality strictly based on the given score rubrics below. If the response is more detailed or lengthy, this is not a disadvantage unless it includes off-topic or irrelevant content.\n",
    "2. After writing feedback, provide a score between 1 and 5 for each evaluation criterion.\n",
    "3. After feedback and scores, provide an overall correctness score (Correct or Incorrect) if the response, in the context of a yes/no question, is correct.\n",
    "4. Format your output as: \"Feedback: {{feedback for each criterion}} [SCORE_FACTUALITY] {{score}} [SCORE_RELEVANCE] {{score}} [SCORE_COMPLETENESS] {{score}} [SCORE_CONFIDENCE] {{score}} [CORRECTNESS] {{Correct or Incorrect}}\"\n",
    "5. Please do not add any other opening, closing, or explanations. Include [SCORE_FACTUALITY], [SCORE_RELEVANCE], [SCORE_COMPLETENESS], [SCORE_CONFIDENCE], and [CORRECTNESS] in your output.\n",
    "\n",
    "### The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "### Response to evaluate:\n",
    "{response}\n",
    "\n",
    "### Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "### Score Rubrics:\n",
    "1. **Factuality**: Is the response correct, accurate, and factual based on the reference answer?\n",
    "   - Score 1: Completely incorrect, inaccurate, and/or not factual.\n",
    "   - Score 2: Mostly incorrect, inaccurate, and/or not factual.\n",
    "   - Score 3: Somewhat correct, accurate, and/or factual.\n",
    "   - Score 4: Mostly correct, accurate, and factual.\n",
    "   - Score 5: Completely correct, accurate, and factual.\n",
    "\n",
    "2. **Relevance**: Does the response stay focused on the instruction and provide relevant information without introducing unnecessary or off-topic content?\n",
    "   - Score 1: Completely irrelevant to the instruction or question.\n",
    "   - Score 2: Mostly irrelevant with some on-topic information.\n",
    "   - Score 3: Somewhat relevant but includes some unnecessary information.\n",
    "   - Score 4: Mostly relevant with little unnecessary information.\n",
    "   - Score 5: Fully relevant and focused on the instruction.\n",
    "\n",
    "3. **Completeness**: Does the response thoroughly cover all parts of the question or instruction without omitting important details?\n",
    "   - Score 1: Completely incomplete, misses all key points.\n",
    "   - Score 2: Misses most key points, partially complete.\n",
    "   - Score 3: Addresses some key points but is incomplete in other aspects.\n",
    "   - Score 4: Addresses most key points with minor omissions.\n",
    "   - Score 5: Fully complete, addresses all key points directly.\n",
    "\n",
    "4. **Confidence**: How confident is the response in providing accurate information based on the reference answer?\n",
    "   - Score 1: Completely unsure or lacking confidence.\n",
    "   - Score 2: Mostly unsure, indicates low confidence.\n",
    "   - Score 3: Somewhat confident but lacks strong evidence.\n",
    "   - Score 4: Mostly confident with some solid backing.\n",
    "   - Score 5: Completely confident, well-supported by evidence.\n",
    "\n",
    "### Feedback:\"\"\"\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages([HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T15:19:43.232025Z",
     "start_time": "2024-10-31T15:19:43.226522Z"
    }
   },
   "id": "f038276cabb454a8",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.prompts.chat import (ChatPromptTemplate, HumanMessagePromptTemplate)\n",
    "import numpy as np\n",
    "\n",
    "# Configure API\n",
    "with open(\"gemini_api_key.txt\") as f:\n",
    "    gemini_api_key = f.read().strip()\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "evaluator = genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages([HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT)])\n",
    "\n",
    "# Function to load test set from an Excel file\n",
    "def load_test_set(file_path: str, rag_flag: bool) -> Dataset:\n",
    "    result_df = pd.read_excel(file_path)\n",
    "    testset = []\n",
    "    for _, row in result_df.iterrows():\n",
    "        question = row['Question']\n",
    "        answer = row['RAG Answer'] if rag_flag else row['Direct Answer']\n",
    "        ground_truth = row['Right Answer']\n",
    "        company = row['Company']\n",
    "        similarity_score = row['Similarity Score']\n",
    "        optimal_index = row['Optimal Index'] if \"Optimal Index\" in result_df else None\n",
    "        testset.append({\n",
    "            \"question\": question,\n",
    "            \"generated_answer\": answer,\n",
    "            \"true_answer\": ground_truth,\n",
    "            \"company\": company,\n",
    "            \"similarity_score\": similarity_score,\n",
    "            \"optimal_index\": optimal_index\n",
    "        })\n",
    "    return testset\n",
    "\n",
    "# Function to extract metrics from evaluation response\n",
    "def extract_evaluation_metrics(eval_response_text):\n",
    "    text = eval_response_text\n",
    "    # Extract feedback (before the first score marker)\n",
    "    feedback = text.split(\"[SCORE_FACTUALITY]\")[0].strip()\n",
    "    # Extract each score and correctness using the markers\n",
    "    factuality_score = int(text.split(\"[SCORE_FACTUALITY]\")[1].split(\"[SCORE_RELEVANCE]\")[0].strip())\n",
    "    relevance_score = int(text.split(\"[SCORE_RELEVANCE]\")[1].split(\"[SCORE_COMPLETENESS]\")[0].strip())\n",
    "    completeness_score = int(text.split(\"[SCORE_COMPLETENESS]\")[1].split(\"[SCORE_CONFIDENCE]\")[0].strip())\n",
    "    confidence_score = int(text.split(\"[SCORE_CONFIDENCE]\")[1].split(\"[CORRECTNESS]\")[0].strip())\n",
    "    correctness = text.split(\"[CORRECTNESS]\")[1].strip()\n",
    "    \n",
    "    return {\n",
    "        \"feedback\": feedback,\n",
    "        \"factuality_score\": factuality_score,\n",
    "        \"relevance_score\": relevance_score,\n",
    "        \"completeness_score\": completeness_score,\n",
    "        \"confidence_score\": confidence_score,\n",
    "        \"correctness\": correctness\n",
    "    }\n",
    "\n",
    "# Function to evaluate a test set and save results\n",
    "def evaluate_and_save(testset, evaluator, test_name: str, save_path: str, rag_flag: bool):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    evaluation_results = []\n",
    "    for experiment in testset:\n",
    "        evaluation_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"]\n",
    "        )\n",
    "        eval_response = evaluator.generate_content(str(evaluation_prompt))\n",
    "        time.sleep(5)  # avoid hitting rate limits\n",
    "        eval_metrics = extract_evaluation_metrics(eval_response.text)\n",
    "        experiment.update({\n",
    "            \"eval_factuality\": eval_metrics[\"factuality_score\"],\n",
    "            \"eval_relevance\": eval_metrics[\"relevance_score\"],\n",
    "            \"eval_completeness\": eval_metrics[\"completeness_score\"],\n",
    "            \"eval_confidence\": eval_metrics[\"confidence_score\"],\n",
    "            \"eval_correctness\": eval_metrics[\"correctness\"],\n",
    "            \"eval_feedback\": eval_metrics[\"feedback\"],\n",
    "            \"mean_similarity_score\": np.mean([float(x) for x in experiment[\"similarity_score\"][1:-1].split(\", \")]),\n",
    "            \"max_similarity_score\": np.max([float(x) for x in experiment[\"similarity_score\"][1:-1].split(\", \")])\n",
    "        })\n",
    "        evaluation_results.append(experiment)\n",
    "    suffix = \"_RAG\" if rag_flag else \"_Direct\"\n",
    "    evaluation_df = pd.DataFrame(evaluation_results)\n",
    "    evaluation_df.to_csv(os.path.join(save_path, f\"{test_name}{suffix}_evaluation.csv\"), index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T15:57:04.043536Z",
     "start_time": "2024-10-31T15:57:04.027733Z"
    }
   },
   "id": "626e8d00f29637d7",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d842cec02ba747e3ab3078bee87e9b2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07958b289e1545f3b0b2d98a4a64107c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7033eed509204b28aac8154b85ed639c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run evaluations on all files in model_responses folder for both RAG and Direct answers\n",
    "model_responses_folder = \"model_responses_test\"\n",
    "evaluation_folder = \"evaluation_test\"\n",
    "for file_name in tqdm(os.listdir(model_responses_folder)):\n",
    "    if file_name.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(model_responses_folder, file_name)\n",
    "        test_name = os.path.splitext(file_name)[0]\n",
    "        for rag_flag in [True, False]:\n",
    "            testset = load_test_set(file_path, rag_flag=rag_flag)\n",
    "            evaluate_and_save(testset, evaluator, test_name, evaluation_folder, rag_flag=rag_flag)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T15:56:30.250875Z",
     "start_time": "2024-10-31T15:56:06.247321Z"
    }
   },
   "id": "60c4cde3a5ef8735",
   "execution_count": 64
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
