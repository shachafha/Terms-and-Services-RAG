{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.prompts.chat import (ChatPromptTemplate, HumanMessagePromptTemplate)\n",
    "\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "evaluator = genai.GenerativeModel(model_name='gemini-1.5-flash')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T17:57:43.791537Z",
     "start_time": "2024-10-16T17:57:43.787249Z"
    }
   },
   "id": "5cc9f2fda78e230f",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"### Task Description:\n",
    "An instruction (might include an input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing multiple evaluation criteria are given.\n",
    "1. Write detailed feedback that assesses the quality of the response strictly based on the given score rubrics below. Additional information is not a disadvantage unless it negatively impacts clarity or relevance.\n",
    "2. After writing feedback, provide a score that is an integer between 1 and 5 for each evaluation criterion.\n",
    "3. After writing the scores, provide an overall correctness score (Correct or Incorrect) if the response, in the context of a yes/no question, is correct.\n",
    "4. The output format should look as follows: \"Feedback: {{write feedback for each criterion}} [SCORE_FACTUALITY] {{score for factuality}} [SCORE_RELEVANCE] {{score for relevance}} [SCORE_COMPLETENESS] {{score for completeness}} [SCORE_CLARITY] {{score for clarity}} [SCORE_CONFIDENCE] {{score for confidence}} [CORRECTNESS] {{Correct or Incorrect}}\"\n",
    "5. Please do not generate any other opening, closing, or explanations. Be sure to include [SCORE_FACTUALITY], [SCORE_RELEVANCE], [SCORE_COMPLETENESS], [SCORE_CLARITY], [SCORE_CONFIDENCE], and [CORRECTNESS] in your output.\n",
    "\n",
    "### The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "### Response to evaluate:\n",
    "{response}\n",
    "\n",
    "### Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "### Score Rubrics:\n",
    "1. **Factuality**: Is the response correct, accurate, and factual based on the reference answer?\n",
    "   - Score 1: Completely incorrect, inaccurate, and/or not factual.\n",
    "   - Score 2: Mostly incorrect, inaccurate, and/or not factual.\n",
    "   - Score 3: Somewhat correct, accurate, and/or factual.\n",
    "   - Score 4: Mostly correct, accurate, and factual.\n",
    "   - Score 5: Completely correct, accurate, and factual.\n",
    "\n",
    "2. **Relevance**: Does the response stay focused on the instruction and provide relevant information without introducing unnecessary or off-topic content?\n",
    "   - Score 1: Completely irrelevant to the instruction or question.\n",
    "   - Score 2: Mostly irrelevant with some on-topic information.\n",
    "   - Score 3: Somewhat relevant but introduces some unnecessary information.\n",
    "   - Score 4: Mostly relevant with little unnecessary information.\n",
    "   - Score 5: Fully relevant and focused on the instruction.\n",
    "\n",
    "3. **Completeness**: Does the response sufficiently address all aspects of the instruction or question without missing key points?\n",
    "   - Score 1: Completely incomplete, misses all key points.\n",
    "   - Score 2: Misses most key points, partially complete.\n",
    "   - Score 3: Addresses some key points but is incomplete in other aspects.\n",
    "   - Score 4: Addresses most key points with minor omissions.\n",
    "   - Score 5: Fully complete, addresses all key points.\n",
    "\n",
    "4. **Clarity**: Is the response clear and easy to understand without being overly complex or ambiguous?\n",
    "   - Score 1: Completely unclear, hard to understand.\n",
    "   - Score 2: Mostly unclear or difficult to follow.\n",
    "   - Score 3: Somewhat clear but may include ambiguous or confusing parts.\n",
    "   - Score 4: Mostly clear with minor clarity issues.\n",
    "   - Score 5: Completely clear and easy to understand.\n",
    "\n",
    "5. **Confidence**: How confident is the response in providing accurate information based on the reference answer?\n",
    "   - Score 1: Completely unsure or lacking confidence.\n",
    "   - Score 2: Mostly unsure, indicates low confidence.\n",
    "   - Score 3: Somewhat confident but lacks strong evidence.\n",
    "   - Score 4: Mostly confident with some solid backing.\n",
    "   - Score 5: Completely confident, well-supported by evidence.\n",
    "\n",
    "### Feedback:\"\"\"\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages([HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T20:25:03.664377Z",
     "start_time": "2024-10-16T20:25:03.658708Z"
    }
   },
   "id": "62a307e3700d6f8e",
   "execution_count": 158
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_test_set(file_path: str, rag_flag) -> Dataset:\n",
    "    \"\"\"\n",
    "    Load responses from an Excel file and transform them into a Dataset object.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the Excel file containing the responses.\n",
    "    - rag_flag (bool): A flag indicating whether to evaluate the RAG Answer.\n",
    "\n",
    "    Returns:\n",
    "    - Dataset: A Dataset object containing the transformed data.\n",
    "    \"\"\"\n",
    "    # Load the Excel file\n",
    "    result_df = pd.read_excel(file_path)\n",
    "    display(result_df)\n",
    "    # Filter and rename the necessary columns\n",
    "    result_df = result_df[['Question', 'Company', 'Context', 'RAG Answer', 'Direct Answer', 'Right Answer']]\n",
    "\n",
    "    # Transform the DataFrame into the desired format\n",
    "    testset = []\n",
    "    for index, row in result_df.iterrows():\n",
    "        question = row['Question']\n",
    "        answer = row['RAG Answer'] if rag_flag else row['Direct Answer']\n",
    "        ground_truth = row['Right Answer'] \n",
    "        testset.append({\n",
    "            \"question\": question,\n",
    "            \"generated_answer\": answer,\n",
    "            \"true_answer\": ground_truth\n",
    "        })\n",
    "\n",
    "    # Create a Dataset object from the data\n",
    "    return Dataset.from_list(testset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T20:12:25.992538Z",
     "start_time": "2024-10-16T20:12:25.987326Z"
    }
   },
   "id": "5235365055f2de34",
   "execution_count": 155
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_evaluation_metrics(eval_response_text):\n",
    "    \"\"\"\n",
    "    Extract feedback, scores, and correctness from evaluation response text.\n",
    "    \n",
    "    Parameters:\n",
    "    eval_response_text (str): The evaluation response text to parse.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the feedback, scores, and correctness.\n",
    "    \"\"\"\n",
    "    text = eval_response_text\n",
    "    \n",
    "    # Extract feedback (before the first score marker)\n",
    "    feedback = text.split(\"[SCORE_FACTUALITY]\")[0].strip()\n",
    "    \n",
    "    # Extract each score and correctness using the markers\n",
    "    factuality_score = int(text.split(\"[SCORE_FACTUALITY]\")[1].split(\"[SCORE_RELEVANCE]\")[0].strip())\n",
    "    relevance_score = int(text.split(\"[SCORE_RELEVANCE]\")[1].split(\"[SCORE_COMPLETENESS]\")[0].strip())\n",
    "    completeness_score = int(text.split(\"[SCORE_COMPLETENESS]\")[1].split(\"[SCORE_CLARITY]\")[0].strip())\n",
    "    clarity_score = int(text.split(\"[SCORE_CLARITY]\")[1].split(\"[SCORE_CONFIDENCE]\")[0].strip())\n",
    "    confidence_score = int(text.split(\"[SCORE_CONFIDENCE]\")[1].split(\"[CORRECTNESS]\")[0].strip())\n",
    "    correctness = text.split(\"[CORRECTNESS]\")[1].strip()\n",
    "    \n",
    "    return {\n",
    "        \"feedback\": feedback,\n",
    "        \"factuality_score\": factuality_score,\n",
    "        \"relevance_score\": relevance_score,\n",
    "        \"completeness_score\": completeness_score,\n",
    "        \"clarity_score\": clarity_score,\n",
    "        \"confidence_score\": confidence_score,\n",
    "        \"correctness\": correctness\n",
    "    }\n",
    "\n",
    "def evaluate(testset, evaluator):\n",
    "    evaluation = []\n",
    "    for experiment in tqdm(testset):\n",
    "        # Create the evaluation prompt with the new metrics\n",
    "        evaluation_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        \n",
    "        # Generate the evaluation response from the evaluator\n",
    "        eval_response = evaluator.generate_content(str(evaluation_prompt))\n",
    "        \n",
    "        try:\n",
    "            eval_metrics = extract_evaluation_metrics(eval_response.text)\n",
    "            # Add the extracted evaluation metrics to the experiment\n",
    "            experiment[\"eval_factuality\"] = eval_metrics[\"factuality_score\"]\n",
    "            experiment[\"eval_relevance\"] = eval_metrics[\"relevance_score\"]\n",
    "            experiment[\"eval_completeness\"] = eval_metrics[\"completeness_score\"]\n",
    "            experiment[\"eval_clarity\"] = eval_metrics[\"clarity_score\"]\n",
    "            experiment[\"eval_confidence\"] = eval_metrics[\"confidence_score\"]\n",
    "            experiment[\"eval_correctness\"] = eval_metrics[\"correctness\"]\n",
    "            experiment[\"eval_feedback\"] = eval_metrics[\"feedback\"]\n",
    "            evaluation.append(experiment)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any unexpected error and assign default values\n",
    "            print(f\"Error processing evaluation: {e}\")\n",
    "    return evaluation\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T20:29:05.990719Z",
     "start_time": "2024-10-16T20:29:05.982044Z"
    }
   },
   "id": "52e0b270ba882144",
   "execution_count": 161
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                            Question  Company  \\\n0      Can tokens in Bonanza be redeemed right away?  Bonanza   \n1  Does Microsoft claim ownership of the content ...     Bing   \n2  are domain names an Intellectual property of G...    GLOVO   \n3    Is manipulating item prices allowed on Bonanza?  Bonanza   \n\n                                             Context  \\\n0  1. 9\\. Tokens may be redeemed immediately or a...   \n1  1. User activity in Bing is governed by the Mi...   \n2  1. 1.7. For the purpose of providing MANDATARY...   \n3  1. You may also wish to consider using a third...   \n\n                                          RAG Answer  \\\n0  Based on the provided Terms and Conditions, **...   \n1  Based on the provided context, the Microsoft T...   \n2  Based on the provided information, the Terms a...   \n3  Based on the provided Terms and Conditions doc...   \n\n                                       Direct Answer  \\\n0  I do not have access to real-time information,...   \n1  I do not have access to real-time information,...   \n2  I do not have access to the specific Terms and...   \n3  I do not have access to real-time information,...   \n\n                                        Right Answer  \n0  Tokens may be redeemed immediately or accrued ...  \n1  Microsoft does not claim ownership of Prompts,...  \n2  All brands, domain names, software and other c...  \n3  The price stated in each Bonanza Item Descript...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Company</th>\n      <th>Context</th>\n      <th>RAG Answer</th>\n      <th>Direct Answer</th>\n      <th>Right Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Can tokens in Bonanza be redeemed right away?</td>\n      <td>Bonanza</td>\n      <td>1. 9\\. Tokens may be redeemed immediately or a...</td>\n      <td>Based on the provided Terms and Conditions, **...</td>\n      <td>I do not have access to real-time information,...</td>\n      <td>Tokens may be redeemed immediately or accrued ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Does Microsoft claim ownership of the content ...</td>\n      <td>Bing</td>\n      <td>1. User activity in Bing is governed by the Mi...</td>\n      <td>Based on the provided context, the Microsoft T...</td>\n      <td>I do not have access to real-time information,...</td>\n      <td>Microsoft does not claim ownership of Prompts,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>are domain names an Intellectual property of G...</td>\n      <td>GLOVO</td>\n      <td>1. 1.7. For the purpose of providing MANDATARY...</td>\n      <td>Based on the provided information, the Terms a...</td>\n      <td>I do not have access to the specific Terms and...</td>\n      <td>All brands, domain names, software and other c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Is manipulating item prices allowed on Bonanza?</td>\n      <td>Bonanza</td>\n      <td>1. You may also wish to consider using a third...</td>\n      <td>Based on the provided Terms and Conditions doc...</td>\n      <td>I do not have access to real-time information,...</td>\n      <td>The price stated in each Bonanza Item Descript...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c95b730bacf04462878dd6b932feb8c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = 'responses.xlsx'\n",
    "test_name = 'rag_spacy_gemini'\n",
    "testset = load_test_set(file_path, rag_flag=True)\n",
    "evaluation = pd.DataFrame(evaluate(testset, evaluator))\n",
    "# evaluation.to_csv(f\"{test_name}_evaluation.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T20:43:56.093998Z",
     "start_time": "2024-10-16T20:43:51.638222Z"
    }
   },
   "id": "4d4dbdef1be1d491",
   "execution_count": 165
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# compute accuracy, precision, recall, f1 score\n",
    "def compute_accuracy(evaluation):\n",
    "    correct = evaluation[evaluation[\"eval_correctness\"] == \"Correct\"]\n",
    "    total = len(evaluation)\n",
    "    correct_count = len(correct)\n",
    "    return correct_count / total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T20:08:28.976258Z",
     "start_time": "2024-10-16T20:08:28.972032Z"
    }
   },
   "id": "ab653941fd9844e1",
   "execution_count": 150
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n",
      "Average Factuality Score: 3.25/5\n",
      "Average Relevance Score: 4.25/5\n",
      "Average Completeness Score: 3.5/5\n",
      "Average Clarity Score: 4.5/5\n",
      "Average Confidence Score: 3.25/5\n",
      "Overall Average Score: 3.75/5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {compute_accuracy(evaluation)}\")\n",
    "# print avg scores\n",
    "\n",
    "print(f\"Average Factuality Score: {evaluation['eval_factuality'].mean()}/5\")\n",
    "print(f\"Average Relevance Score: {evaluation['eval_relevance'].mean()}/5\")\n",
    "print(f\"Average Completeness Score: {evaluation['eval_completeness'].mean()}/5\")\n",
    "print(f\"Average Clarity Score: {evaluation['eval_clarity'].mean()}/5\")\n",
    "print(f\"Average Confidence Score: {evaluation['eval_confidence'].mean()}/5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T20:47:34.893208Z",
     "start_time": "2024-10-16T20:47:34.884888Z"
    }
   },
   "id": "91a26c640fa6a98d",
   "execution_count": 170
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
