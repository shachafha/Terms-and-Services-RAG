{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T12:15:09.076163Z",
     "start_time": "2024-09-26T12:15:09.072558Z"
    }
   },
   "id": "61b772115f794890",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "index_name = \"semantic-200-index\"\n",
    "chunk_size = 200\n",
    "embedding_model = \"all-MiniLM-L6-v2\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T12:15:09.979066Z",
     "start_time": "2024-09-26T12:15:09.975545Z"
    }
   },
   "id": "8e8d6ba10518d3df",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "data = \"only_english_data\"\n",
    "# Load API keys\n",
    "with open(\"pinecone_api_key.txt\") as f:\n",
    "    PINECONE_API_KEY = f.read().strip()\n",
    "\n",
    "# Load the models\n",
    "sentence_transformer_model = SentenceTransformer(embedding_model)\n",
    "dimension = sentence_transformer_model.get_sentence_embedding_dimension() \n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(\n",
    "        api_key=PINECONE_API_KEY\n",
    "    )\n",
    "def create_index(pc, index_name, dimension):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            ))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-26T12:15:11.178847Z",
     "start_time": "2024-09-26T12:15:10.694614Z"
    }
   },
   "id": "initial_id",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions\n",
    "def load_data(directory: str):\n",
    "    \"\"\"Load all text files from a directory and its subdirectories.\"\"\"\n",
    "    documents = []\n",
    "    company_names = []\n",
    "    for foldername, _, filenames in os.walk(directory):\n",
    "        company_name = os.path.basename(foldername)\n",
    "        if company_name == 'only_english_data':\n",
    "            continue\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".md\"):\n",
    "                filepath = os.path.join(foldername, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    documents.append(f.read())\n",
    "                    company_names.append(company_name)  # Add company name for each document\n",
    "    return documents, company_names\n",
    "\n",
    "def simple_chunk_data(documents: list, chunk_size: int, company_names: list):\n",
    "    \"\"\"Split documents into smaller chunks based on word count and prepend company names.\"\"\"\n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Split the document into words\n",
    "        words = doc.split()\n",
    "        \n",
    "        # Create chunks based on word count\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "            chunks_company.append(company)\n",
    "            \n",
    "    return chunks,chunks_company\n",
    "\n",
    "def overlapping_chunking(documents: list, chunk_size: int, overlap_size: int, company_names: list):\n",
    "    \"\"\"Create overlapping chunks based on word count and prepend company names.\"\"\"\n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Split the document into words\n",
    "        words = doc.split()\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if chunk:  # Ensure the chunk is not empty\n",
    "                chunks.append(chunk.strip())\n",
    "                chunks_company.append(company)\n",
    "                \n",
    "    return chunks, chunks_company\n",
    "\n",
    "def semantic_chunking(documents: list, company_names: list, model_embedding_name: str):\n",
    "    \"\"\"Split documents into smaller chunks using semantic chunking and prepend company names.\"\"\"\n",
    "    \n",
    "    # Create HuggingFaceEmbeddings wrapper for SentenceTransformer\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_embedding_name)\n",
    "    \n",
    "    # Create SemanticChunker\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings,\n",
    "        breakpoint_threshold_type='percentile',\n",
    "        breakpoint_threshold_amount=90\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Use SemanticChunker to split the document\n",
    "        doc_chunks = text_splitter.split_text(doc)\n",
    "        \n",
    "        # Add chunks and corresponding company names\n",
    "        chunks.extend(doc_chunks)\n",
    "        chunks_company.extend([company] * len(doc_chunks))\n",
    "    \n",
    "    return chunks, chunks_company\n",
    "\n",
    "def embed_text(texts: list):\n",
    "    \"\"\"Embed texts using either Cohere or SentenceTransformer.\"\"\"\n",
    "    return sentence_transformer_model.encode(texts, convert_to_tensor=True).tolist()\n",
    "\n",
    "def upsert_index(index, embeddings, metadata, company_names, batch_size=100):\n",
    "    \"\"\"Insert embeddings into Pinecone in batches with metadata.\"\"\"\n",
    "    batch = []\n",
    "    \n",
    "    for idx, (emb, md, cn) in enumerate(zip(embeddings, metadata, company_names)):\n",
    "        vector = {\"id\": str(idx), \"values\": emb, \"metadata\": {\"text\": md, \"company_name\": cn}}\n",
    "        batch.append(vector)\n",
    "        \n",
    "        # When batch is full, upsert it\n",
    "        if len(batch) == batch_size:\n",
    "            index.upsert(vectors=batch)\n",
    "            batch = []  # Clear the batch\n",
    "\n",
    "    # Upsert any remaining vectors\n",
    "    if batch:\n",
    "        index.upsert(vectors=batch)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T12:17:39.498043Z",
     "start_time": "2024-09-26T12:17:39.485297Z"
    }
   },
   "id": "8c75e0f0c80c3119",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load and embedd Data\n",
    "documents, company_names = load_data(data)   \n",
    "chunks, company_names_chunks = semantic_chunking(documents, company_names,embedding_model)\n",
    "embeddings = embed_text(chunks)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T12:21:57.479408Z",
     "start_time": "2024-09-26T12:17:41.010064Z"
    }
   },
   "id": "3997f8dbda8a5bd6",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# index the data\n",
    "create_index(pc, index_name, dimension)\n",
    "index = pc.Index(index_name)\n",
    "upsert_index(index, embeddings, chunks, company_names_chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T12:24:38.384427Z",
     "start_time": "2024-09-26T12:24:02.980980Z"
    }
   },
   "id": "f87929876fc166e8",
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
