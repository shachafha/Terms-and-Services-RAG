{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 19:46:29.223628: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-11 19:46:29.223669: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-11 19:46:29.225033: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-11 19:46:29.232917: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 19:46:30.444867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/anaconda/envs/azureml_py38_PT_and_TF/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import SpacyTextSplitter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T16:46:32.386722Z",
     "start_time": "2024-10-11T16:46:24.686837Z"
    }
   },
   "id": "61b772115f794890",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "index_name = \"document-structure-index\"\n",
    "chunk_size = 200\n",
    "embedding_model = \"all-MiniLM-L6-v2\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T16:46:59.037469Z",
     "start_time": "2024-10-11T16:46:59.030634Z"
    }
   },
   "id": "8e8d6ba10518d3df",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_PT_and_TF/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = \"only_english_data\"\n",
    "# Load API keys\n",
    "with open(\"pinecone_api_key.txt\") as f:\n",
    "    PINECONE_API_KEY = f.read().strip()\n",
    "\n",
    "# Load the models\n",
    "sentence_transformer_model = SentenceTransformer(embedding_model)\n",
    "dimension = sentence_transformer_model.get_sentence_embedding_dimension() \n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(\n",
    "        api_key=PINECONE_API_KEY\n",
    "    )\n",
    "def create_index(pc, index_name, dimension):\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            ))"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-11T16:47:01.215326Z",
     "start_time": "2024-10-11T16:47:00.433575Z"
    }
   },
   "id": "initial_id",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def load_data(directory: str):\n",
    "    \"\"\"Load all text files from a directory and its subdirectories.\"\"\"\n",
    "    documents = []\n",
    "    company_names = []\n",
    "    for foldername, _, filenames in os.walk(directory):\n",
    "        company_name = os.path.basename(foldername)\n",
    "        if company_name == 'only_english_data':\n",
    "            continue\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".md\"):\n",
    "                filepath = os.path.join(foldername, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    documents.append(f.read())\n",
    "                    company_names.append(company_name)  # Add company name for each document\n",
    "    return documents, company_names\n",
    "\n",
    "def simple_chunk_data(documents: list, chunk_size: int, company_names: list):\n",
    "    \"\"\"Split documents into smaller chunks based on word count and prepend company names.\"\"\"\n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Split the document into words\n",
    "        words = doc.split()\n",
    "        \n",
    "        # Create chunks based on word count\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "            chunks_company.append(company)\n",
    "            \n",
    "    return chunks,chunks_company\n",
    "\n",
    "def overlapping_chunking(documents: list, chunk_size: int, overlap_size: int, company_names: list):\n",
    "    \"\"\"Create overlapping chunks based on word count and prepend company names.\"\"\"\n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Split the document into words\n",
    "        words = doc.split()\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if chunk:  # Ensure the chunk is not empty\n",
    "                chunks.append(chunk.strip())\n",
    "                chunks_company.append(company)\n",
    "                \n",
    "    return chunks, chunks_company\n",
    "\n",
    "def semantic_chunking(documents: list, company_names: list, model_embedding_name: str):\n",
    "    \"\"\"Split documents into smaller chunks using semantic chunking and prepend company names.\"\"\"\n",
    "    \n",
    "    # Create HuggingFaceEmbeddings wrapper for SentenceTransformer\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_embedding_name)\n",
    "    \n",
    "    # Create SemanticChunker\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings,\n",
    "        breakpoint_threshold_type='percentile',\n",
    "        breakpoint_threshold_amount=90\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Use SemanticChunker to split the document\n",
    "        doc_chunks = text_splitter.split_text(doc)\n",
    "        \n",
    "        # Add chunks and corresponding company names\n",
    "        chunks.extend(doc_chunks)\n",
    "        chunks_company.extend([company] * len(doc_chunks))\n",
    "    \n",
    "    return chunks, chunks_company\n",
    "\n",
    "def spacy_chunking(documents: list, company_names: list, chunk_size: int=1000, overlap_size: int=500):\n",
    "    \"\"\"Split documents into smaller chunks using SpacyTextSplitter and prepend company names.\"\"\"\n",
    "    \n",
    "    # Create SpacyTextSplitter\n",
    "    text_splitter = SpacyTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap_size\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    chunks_company = []\n",
    "    \n",
    "    for doc, company in zip(documents, company_names):\n",
    "        # Use SpacyTextSplitter to split the document\n",
    "        doc_chunks = text_splitter.split_text(doc)\n",
    "        \n",
    "        # Add chunks and corresponding company names\n",
    "        chunks.extend(doc_chunks)\n",
    "        chunks_company.extend([company] * len(doc_chunks))\n",
    "    \n",
    "    return chunks, chunks_company\n",
    "\n",
    "\n",
    "\n",
    "def embed_text(texts: list):\n",
    "    \"\"\"Embed texts using either Cohere or SentenceTransformer.\"\"\"\n",
    "    return sentence_transformer_model.encode(texts, convert_to_tensor=True).tolist()\n",
    "\n",
    "def upsert_index(index, embeddings, metadata, company_names, batch_size=100):\n",
    "    \"\"\"Insert embeddings into Pinecone in batches with metadata.\"\"\"\n",
    "    batch = []\n",
    "    \n",
    "    for idx, (emb, md, cn) in enumerate(zip(embeddings, metadata, company_names)):\n",
    "        vector = {\"id\": str(idx), \"values\": emb, \"metadata\": {\"text\": md, \"company_name\": cn}}\n",
    "        batch.append(vector)\n",
    "        \n",
    "        # When batch is full, upsert it\n",
    "        if len(batch) == batch_size:\n",
    "            index.upsert(vectors=batch)\n",
    "            batch = []  # Clear the batch\n",
    "\n",
    "    # Upsert any remaining vectors\n",
    "    if batch:\n",
    "        index.upsert(vectors=batch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T16:47:06.550351Z",
     "start_time": "2024-10-11T16:47:06.534690Z"
    }
   },
   "id": "8c75e0f0c80c3119",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load and embedd Data\n",
    "documents, company_names = load_data(data)   \n",
    "chunks, company_names_chunks = spacy_chunking(documents, company_names)\n",
    "embeddings = embed_text(chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T16:50:40.202852Z",
     "start_time": "2024-10-11T16:50:40.137142Z"
    }
   },
   "id": "3997f8dbda8a5bd6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# index the data\n",
    "create_index(pc, index_name, dimension)\n",
    "index = pc.Index(index_name)\n",
    "upsert_index(index, embeddings, chunks, company_names_chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T15:22:46.027130Z",
     "start_time": "2024-10-08T15:18:23.748105Z"
    }
   },
   "id": "f87929876fc166e8",
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
